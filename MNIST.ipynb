{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80074a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rc('font', size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da7ab767",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9210509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "checkpoint_dir = os.path.join(cur_dir, 'checkpoints', 'mnist_cnn_seq')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'mnist_cnn_seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c29bdeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalization\n",
    "X_train = X_train.astype(np.float32) / 255.\n",
    "X_test = X_test.astype(np.float32) / 255.\n",
    "\n",
    "# Convert it to 4D array (or we can use np.expand_dims for dimension expansion)\n",
    "X_train = X_train[..., tf.newaxis]\n",
    "X_test = X_test[..., tf.newaxis]\n",
    "\n",
    "# one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Build dataset pipeline\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=100000).batch(batch_size)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1413dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_Sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               524544    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 619,786\n",
      "Trainable params: 619,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential(name='CNN_Sequential')\n",
    "    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.keras.activations.relu,\n",
    "                                     padding='SAME', input_shape=(28, 28, 1)))\n",
    "    model.add(tf.keras.layers.MaxPool2D(padding='SAME'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.keras.activations.relu,\n",
    "                                     padding='SAME'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(padding='SAME'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation=tf.keras.activations.relu,\n",
    "                                     padding='SAME'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(padding='SAME'))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(256, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dropout(0.4))\n",
    "    model.add(tf.keras.layers.Dense(10))\n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3840922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, images, labels):\n",
    "    logits = model(images, training=True)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    return loss\n",
    "\n",
    "# Gradient Function\n",
    "def grad(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, images, labels)\n",
    "    return tape.gradient(loss, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02d8822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, images, labels):\n",
    "    logits = model(images, training=False)\n",
    "    correct_predict = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = tf.train.Checkpoint(cnn=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e857d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.17913757 train acc: 0.9578 test acc: 0.9870\n",
      "Epoch: 2 loss: 0.04863174 train acc: 0.9896 test acc: 0.9899\n",
      "Epoch: 3 loss: 0.03315681 train acc: 0.9932 test acc: 0.9904\n",
      "Epoch: 4 loss: 0.02572735 train acc: 0.9948 test acc: 0.9925\n",
      "Epoch: 5 loss: 0.01919295 train acc: 0.9965 test acc: 0.9908\n",
      "Epoch: 6 loss: 0.01603331 train acc: 0.9971 test acc: 0.9917\n",
      "Epoch: 7 loss: 0.01319477 train acc: 0.9978 test acc: 0.9946\n",
      "Epoch: 8 loss: 0.01167356 train acc: 0.9981 test acc: 0.9929\n",
      "Epoch: 9 loss: 0.01066297 train acc: 0.9985 test acc: 0.9933\n",
      "Epoch: 10 loss: 0.00778059 train acc: 0.9988 test acc: 0.9927\n",
      "Epoch: 11 loss: 0.00698124 train acc: 0.9991 test acc: 0.9916\n",
      "Epoch: 12 loss: 0.00653007 train acc: 0.9991 test acc: 0.9932\n",
      "Epoch: 13 loss: 0.00609690 train acc: 0.9992 test acc: 0.9929\n",
      "Epoch: 14 loss: 0.00555209 train acc: 0.9994 test acc: 0.9942\n",
      "Epoch: 15 loss: 0.00506163 train acc: 0.9993 test acc: 0.9935\n"
     ]
    }
   ],
   "source": [
    "for e in range(training_epochs):\n",
    "    avg_loss = 0.\n",
    "    avg_train_acc = 0.\n",
    "    avg_test_acc = 0.\n",
    "    train_step = 0\n",
    "    test_step = 0\n",
    "    \n",
    "    for images, labels in train_ds:\n",
    "        grads = grad(model, images, labels)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        loss = loss_fn(model, images, labels)\n",
    "        acc = evaluate(model, images, labels)\n",
    "        avg_loss = avg_loss + loss\n",
    "        avg_train_acc = avg_train_acc + acc\n",
    "        train_step += 1\n",
    "    avg_loss = avg_loss / train_step\n",
    "    avg_train_acc = avg_train_acc / train_step\n",
    "    \n",
    "    for images, labels in test_ds:\n",
    "        acc = evaluate(model, images, labels)\n",
    "        avg_test_acc = avg_test_acc + acc\n",
    "        test_step += 1\n",
    "    avg_test_acc = avg_test_acc / test_step\n",
    "    \n",
    "    print(\"Epoch: {}\".format(e + 1),\n",
    "          \"loss: {:.8f}\".format(avg_loss),\n",
    "          \"train acc: {:.4f}\".format(avg_train_acc),\n",
    "          \"test acc: {:.4f}\".format(avg_test_acc))\n",
    "    \n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "888cb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_functional():\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "    conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', \n",
    "                                   activation=tf.keras.activations.relu)(inputs)\n",
    "    pool1 = tf.keras.layers.MaxPool2D(padding='SAME')(conv1)\n",
    "    conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding='SAME',\n",
    "                                   activation=tf.keras.activations.relu)(pool1)\n",
    "    pool2 = tf.keras.layers.MaxPool2D(padding='SAME')(conv2)\n",
    "    conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), padding='SAME',\n",
    "                                   activation=tf.keras.activations.relu)(pool2)\n",
    "    pool3 = tf.keras.layers.MaxPool2D(padding='SAME')(conv3)\n",
    "    pool3_flat = tf.keras.layers.Flatten()(pool3)\n",
    "    dense4 = tf.keras.layers.Dense(units=256, activation=tf.keras.activations.relu)(pool3_flat)\n",
    "    drop4 = tf.keras.layers.Dropout(rate=0.4)(dense4)\n",
    "    logits = tf.keras.layers.Dense(units=10)(drop4)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb247975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               524544    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 619,786\n",
      "Trainable params: 619,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model_functional()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c297046",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28, 28, 256))\n",
    "conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(1, 1), padding='SAME', activation=tf.keras.activations.relu)(inputs)\n",
    "conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation=tf.keras.activations.relu)(conv1)\n",
    "conv3 = tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1), padding='SAME')(conv2)\n",
    "# skip connection\n",
    "add3 = tf.keras.layers.add([conv3, inputs])\n",
    "relu3 = tf.keras.activations.relu(add3)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=relu3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10025409",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='SAME',\n",
    "                                            activation=tf.keras.activations.relu)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(padding='SAME')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding='SAME',\n",
    "                                            activation=tf.keras.activations.relu)\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(padding='SAME')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), padding='SAME',\n",
    "                                            activation=tf.keras.activations.relu)\n",
    "        self.pool3 = tf.keras.layers.MaxPool2D(padding='SAME')\n",
    "        self.pool3_flat = tf.keras.layers.Flatten()\n",
    "        self.dense4 = tf.keras.layers.Dense(units=256, activation=tf.keras.activations.relu)\n",
    "        self.drop4 = tf.keras.layers.Dropout(rate=0.4)\n",
    "        self.dense5 = tf.keras.layers.Dense(units=10)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        net = self.conv1(inputs)\n",
    "        net = self.pool1(net)\n",
    "        net = self.conv2(net)\n",
    "        net = self.pool2(net)\n",
    "        net = self.conv3(net)\n",
    "        net = self.pool3(net)\n",
    "        net = self.pool3_flat(net)\n",
    "        net = self.dense4(net)\n",
    "        net = self.drop4(net)\n",
    "        net = self.dense5(net)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fbe28fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0831d757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          multiple                  320       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          multiple                  18496     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  multiple                 0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          multiple                  73856     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  multiple                 0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             multiple                  524544    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             multiple                  2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 619,786\n",
      "Trainable params: 619,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build(input_shape=(1, 28, 28, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ace31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='SAME',\n",
    "                                            activation=tf.keras.activations.relu)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(padding='SAME')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding='SAME',\n",
    "                                            activation=tf.keras.activations.relu)\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(padding='SAME')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), padding='SAME',\n",
    "                                            activation=tf.keras.activations.relu)\n",
    "        self.pool3 = tf.keras.layers.MaxPool2D(padding='SAME')\n",
    "        self.pool3_flat = tf.keras.layers.Flatten()\n",
    "        self.dense4 = tf.keras.layers.Dense(units=256, activation=tf.keras.activations.relu)\n",
    "        self.drop4 = tf.keras.layers.Dropout(rate=0.4)\n",
    "        self.dense5 = tf.keras.layers.Dense(units=10)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        net = self.conv1(inputs)\n",
    "        net = self.pool1(net)\n",
    "        net = self.conv2(net)\n",
    "        net = self.pool2(net)\n",
    "        net = self.conv3(net)\n",
    "        net = self.pool3(net)\n",
    "        net = self.pool3_flat(net)\n",
    "        net = self.dense4(net)\n",
    "        net = self.drop4(net)\n",
    "        net = self.dense5(net)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ee01428",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for m in range(3):\n",
    "    models.append(CNNModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86992b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, images, labels):\n",
    "    logits = model(images, training=True)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    return loss\n",
    "\n",
    "# Gradient Function\n",
    "def grad(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, images, labels)\n",
    "    return tape.gradient(loss, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d296d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "def evaluate(models, images, labels):\n",
    "    predicts = tf.zeros_like(labels)\n",
    "    for model in models:\n",
    "        logits = model(images, training=False)\n",
    "        predicts += logits\n",
    "    correct_predict = tf.equal(tf.argmax(predicts, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "checkpoints = []\n",
    "for model in models:\n",
    "    checkpoints.append(tf.train.Checkpoint(cnn=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "482bb921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.15972973 Train Accuracy: 0.9636 Test Accuracy: 0.9899\n",
      "Epoch: 2 Loss: 0.03789086 Train Accuracy: 0.9933 Test Accuracy: 0.9905\n",
      "Epoch: 3 Loss: 0.02561753 Train Accuracy: 0.9959 Test Accuracy: 0.9932\n",
      "Epoch: 4 Loss: 0.01906417 Train Accuracy: 0.9974 Test Accuracy: 0.9942\n",
      "Epoch: 5 Loss: 0.01414697 Train Accuracy: 0.9983 Test Accuracy: 0.9914\n",
      "Epoch: 6 Loss: 0.01151887 Train Accuracy: 0.9988 Test Accuracy: 0.9942\n",
      "Epoch: 7 Loss: 0.01049820 Train Accuracy: 0.9992 Test Accuracy: 0.9949\n",
      "Epoch: 8 Loss: 0.00837994 Train Accuracy: 0.9993 Test Accuracy: 0.9938\n",
      "Epoch: 9 Loss: 0.00733276 Train Accuracy: 0.9996 Test Accuracy: 0.9938\n",
      "Epoch: 10 Loss: 0.00654636 Train Accuracy: 0.9996 Test Accuracy: 0.9943\n",
      "Epoch: 11 Loss: 0.00558788 Train Accuracy: 0.9998 Test Accuracy: 0.9944\n",
      "Epoch: 12 Loss: 0.00518030 Train Accuracy: 0.9998 Test Accuracy: 0.9938\n",
      "Epoch: 13 Loss: 0.00446692 Train Accuracy: 0.9999 Test Accuracy: 0.9945\n",
      "Epoch: 14 Loss: 0.00396374 Train Accuracy: 0.9998 Test Accuracy: 0.9944\n",
      "Epoch: 15 Loss: 0.00441842 Train Accuracy: 0.9999 Test Accuracy: 0.9949\n"
     ]
    }
   ],
   "source": [
    "for e in range(training_epochs):\n",
    "    avg_loss = 0.\n",
    "    avg_train_acc = 0.\n",
    "    avg_test_acc = 0.\n",
    "    train_step = 0\n",
    "    test_step = 0\n",
    "    \n",
    "    for images, labels in train_ds:\n",
    "        for model in models:\n",
    "            grads = grad(model, images, labels)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            loss = loss_fn(model, images, labels)\n",
    "            avg_loss += loss / 3\n",
    "        acc = evaluate(models, images, labels)\n",
    "        avg_train_acc += acc\n",
    "        train_step += 1\n",
    "    avg_loss = avg_loss / train_step\n",
    "    avg_train_acc = avg_train_acc / train_step\n",
    "    \n",
    "    for images, labels in test_ds:\n",
    "        acc = evaluate(models, images, labels)\n",
    "        avg_test_acc += acc\n",
    "        test_step += 1\n",
    "    avg_test_acc = avg_test_acc / test_step\n",
    "    \n",
    "    print(\"Epoch: {}\".format(e + 1),\n",
    "          \"Loss: {:.8f}\".format(avg_loss),\n",
    "          \"Train Accuracy: {:.4f}\".format(avg_train_acc),\n",
    "          \"Test Accuracy: {:.4f}\".format(avg_test_acc))\n",
    "    \n",
    "    for idx, checkpoint in enumerate(checkpoints):\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix+'-{}'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d1c2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "import random\n",
    "\n",
    "def data_augmentation(images, labels):\n",
    "    aug_images = []\n",
    "    aug_labels = []\n",
    "    \n",
    "    for image, label in zip(images, labels):\n",
    "        aug_images.append(image)\n",
    "        aug_labels.append(label)\n",
    "        \n",
    "        # Background image for filling empty pixel\n",
    "        bg_value = np.median(image)\n",
    "        for _ in range(4):\n",
    "            # Rotation\n",
    "            rot_image = ndimage.rotate(image, angle=random.randint(-15, 15), \n",
    "                                       reshape=False, cval=bg_value)\n",
    "            # Shift\n",
    "            shift_image = ndimage.shift(rot_image, shift=np.random.randint(-2, 2, 2), \n",
    "                                        cval=bg_value)\n",
    "            \n",
    "            aug_images.append(shift_image)\n",
    "            aug_labels.append(label)\n",
    "    aug_images = np.array(aug_images)\n",
    "    aug_labels = np.array(aug_labels)\n",
    "    return aug_images, aug_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e7d9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train, y_train = data_augmentation(X_train, y_train)\n",
    "\n",
    "# Convert numpy float type and normalize it\n",
    "X_train = X_train.astype(np.float32) / 255.\n",
    "X_test = X_test.astype(np.float32) / 255.\n",
    "\n",
    "# Convert it to 4D array (or we can use np.expand_dims for dimension expansion)\n",
    "X_train = X_train[..., tf.newaxis]\n",
    "X_test = X_test[..., tf.newaxis]\n",
    "\n",
    "# one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Build dataset pipeline\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=500000).batch(batch_size)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10200d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNRelu(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_size=(3, 3), strides=1, padding='SAME'):\n",
    "        super(ConvBNRelu, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, \n",
    "                                           strides=strides, padding=padding,\n",
    "                                           kernel_initializer='glorot_normal')\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        net = self.conv(inputs)\n",
    "        net = self.bn(net)\n",
    "        return tf.keras.activations.relu(net)\n",
    "    \n",
    "class DenseBNRelu(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(DenseBNRelu, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(units=units, kernel_initializer='glorot_normal')\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        net = self.dense(inputs)\n",
    "        net = self.bn(net)\n",
    "        return tf.keras.activations.relu(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb6653c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = ConvBNRelu(filters=32, kernel_size=(3, 3), padding='SAME')\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(padding='SAME')\n",
    "        self.conv2 = ConvBNRelu(filters=64, kernel_size=(3, 3), padding='SAME')\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(padding='SAME')\n",
    "        self.conv3 = ConvBNRelu(filters=128, kernel_size=(3, 3), padding='SAME')\n",
    "        self.pool3 = tf.keras.layers.MaxPool2D(padding='SAME')\n",
    "        self.pool3_flat = tf.keras.layers.Flatten()\n",
    "        self.dense4 = DenseBNRelu(units=256)\n",
    "        self.drop4 = tf.keras.layers.Dropout(rate=0.4)\n",
    "        self.dense5 = tf.keras.layers.Dense(units=10, kernel_initializer='glorot_normal')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        net = self.conv1(inputs)\n",
    "        net = self.pool1(net)\n",
    "        net = self.conv2(net)\n",
    "        net = self.pool2(net)\n",
    "        net = self.conv3(net)\n",
    "        net = self.pool3(net)\n",
    "        net = self.pool3_flat(net)\n",
    "        net = self.dense4(net)\n",
    "        net = self.drop4(net)\n",
    "        return self.dense5(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e864d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for _ in range(5):\n",
    "    models.append(CNNModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cdf25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, images, labels):\n",
    "    logits = model(images, training=True)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    return loss\n",
    "\n",
    "# Gradient Function\n",
    "def grad(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, images, labels)\n",
    "    return tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(models, images, labels):\n",
    "    predicts = tf.zeros_like(labels)\n",
    "    for model in models:\n",
    "        logits = model(images, training=False)\n",
    "        predicts += logits\n",
    "    correct_predict = tf.equal(tf.argmax(predicts, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c70d1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decay = tf.keras.optimizers.schedules.ExponentialDecay(learning_rate, \n",
    "                                          decay_steps=X_train.shape[0] / batch_size * 5 * 5,\n",
    "                                          decay_rate=0.5,\n",
    "                                          staircase=True)\n",
    "\n",
    "# Optimizer with learning rate scheduler\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_decay)\n",
    "\n",
    "# Checkpoint\n",
    "checkpoints = []\n",
    "for model in models:\n",
    "    checkpoints.append(tf.train.Checkpoint(cnn=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b1010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.08116379 Train Accuracy: 0.9222 Test Accuracy: 0.9944\n",
      "Epoch: 2 Loss: 0.03428531 Train Accuracy: 0.9963 Test Accuracy: 0.9947\n"
     ]
    }
   ],
   "source": [
    "for e in range(training_epochs):\n",
    "    avg_loss = 0.\n",
    "    avg_train_acc = 0.\n",
    "    avg_test_acc = 0.\n",
    "    train_step = 0\n",
    "    test_step = 0\n",
    "    \n",
    "    for images, labels in train_ds:\n",
    "        for model in models:\n",
    "            grads = grad(model, images, labels)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            loss = loss_fn(model, images, labels)\n",
    "            avg_loss += loss / 3\n",
    "        acc = evaluate(models, images, labels)\n",
    "        avg_train_acc += acc\n",
    "        train_step += 1\n",
    "    avg_loss = avg_loss / train_step\n",
    "    avg_train_acc = avg_train_acc / train_step\n",
    "    \n",
    "    for images, labels in test_ds:\n",
    "        acc = evaluate(models, images, labels)\n",
    "        avg_test_acc += acc\n",
    "        test_step += 1\n",
    "    avg_test_acc = avg_test_acc / test_step\n",
    "    \n",
    "    print(\"Epoch: {}\".format(e + 1),\n",
    "          \"Loss: {:.8f}\".format(avg_loss),\n",
    "          \"Train Accuracy: {:.4f}\".format(avg_train_acc),\n",
    "          \"Test Accuracy: {:.4f}\".format(avg_test_acc))\n",
    "    \n",
    "    for idx, checkpoint in enumerate(checkpoints):\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix+'-{}'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d9c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
